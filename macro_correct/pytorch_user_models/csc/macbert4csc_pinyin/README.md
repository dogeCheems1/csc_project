# MacBERT + æ‹¼éŸ³èåˆä¸­æ–‡çº é”™æ¨¡å‹

## ğŸ“Œ æ ¸å¿ƒæ”¹è¿›

æœ¬æ¨¡å‹åœ¨ MacBERT åŸºç¡€ä¸Šèåˆäº†**æ‹¼éŸ³ç‰¹å¾**ï¼Œè§£å†³äº†ä»¥ä¸‹é—®é¢˜ï¼š

1. âœ… **ç»´åº¦å¯¹é½**ï¼š`text_features` å’Œ `pinyin_features` éƒ½æ˜¯ `[B, L, H]`ï¼Œå®Œå…¨å¯¹é½
2. âœ… **Mask å¤„ç†**ï¼šä½¿ç”¨ `pack_padded_sequence` é¿å…å¡«å……å™ªéŸ³æ±¡æŸ“ GRU ç¼–ç 
3. âœ… **æ¢¯åº¦å›ä¼ **ï¼šèåˆå±‚çš„æ¢¯åº¦å¯ä»¥åŒæ—¶æ›´æ–° BERT å’Œ GRU å‚æ•°

---

## ğŸš€ ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1ï¼šé¢„è®¡ç®—æ‹¼éŸ³ï¼ˆæ¨èï¼Œæ€§èƒ½æœ€ä¼˜ï¼‰

```python
from transformers import AutoTokenizer
from macro_correct.pytorch_user_models.csc.macbert4csc_pinyin.graph import Macbert4CSCWithPinyin

# åˆå§‹åŒ–æ¨¡å‹
model = Macbert4CSCWithPinyin(config, csc_config)
tokenizer = AutoTokenizer.from_pretrained(config.pretrained_model_name_or_path)

# æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šæå‰è®¡ç®—æ‹¼éŸ³
texts = ["æˆ‘çˆ±ä¸­å›½", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"]
input_ids = tokenizer(texts, padding=True, return_tensors="pt")["input_ids"]
pinyin_ids, pinyin_lengths = model.text_to_pinyin_ids(texts, input_ids.shape[1])

# è®­ç»ƒ/æ¨ç†
outputs = model(
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels,
    pinyin_ids=pinyin_ids,        # â­ é¢„è®¡ç®—çš„æ‹¼éŸ³ID
    pinyin_lengths=pinyin_lengths  # â­ æ‹¼éŸ³é•¿åº¦ï¼ˆç”¨äºMaskï¼‰
)
```

### æ–¹å¼ 2ï¼šåŠ¨æ€è®¡ç®—æ‹¼éŸ³ï¼ˆç®€å•ï¼Œä½†ä¼šæ‹–æ…¢é€Ÿåº¦ï¼‰

```python
# ç›´æ¥ä¼ å…¥åŸå§‹æ–‡æœ¬ï¼Œæ¨¡å‹å†…éƒ¨ä¼šè‡ªåŠ¨è®¡ç®—æ‹¼éŸ³
outputs = model(
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels,
    texts=["æˆ‘çˆ±ä¸­å›½", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"]  # â­ ä¼ å…¥åŸå§‹æ–‡æœ¬
)
```

---

## ğŸ”§ å…³é”®å‚æ•°è¯´æ˜

### 1. FusionLayer èåˆç­–ç•¥

```python
# åœ¨åˆå§‹åŒ–æ—¶é€‰æ‹©èåˆæ–¹å¼
self.fusion_layer = FusionLayer(
    hidden_size=768,
    fusion_type="gate",  # å¯é€‰: "gate", "attention", "bilinear", "add"
    dropout=0.1
)
```

| èåˆæ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|---------|
| `gate` | åŠ¨æ€å­¦ä¹ æƒé‡ï¼Œçµæ´» | å‚æ•°ç¨å¤š | **æ¨è**ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯ |
| `attention` | æ•æ‰é•¿è·ç¦»ä¾èµ– | è®¡ç®—é‡å¤§ | é•¿æ–‡æœ¬çº é”™ |
| `bilinear` | å¼ºäº¤äº’èƒ½åŠ› | å‚æ•°æœ€å¤š | æ•°æ®å……è¶³æ—¶ |
| `add` | ç®€å•å¿«é€Ÿ | è¡¨è¾¾èƒ½åŠ›å¼± | Baseline å¯¹æ¯” |

### 2. æŸå¤±å‡½æ•°æƒé‡

```python
# åœ¨ csc_config ä¸­è®¾ç½®
csc_config.loss_det_rate = 0.3  # æ£€æµ‹æŸå¤±æƒé‡ï¼ˆ0.3è¡¨ç¤º30%æ£€æµ‹+70%çº æ­£ï¼‰
```

**è°ƒå‚å»ºè®®**ï¼š
- å¦‚æœæ¨¡å‹**æ¼æ£€**ä¸¥é‡ï¼ˆè¯¥æ£€æµ‹çš„æ²¡æ£€æµ‹å‡ºæ¥ï¼‰â†’ å¢å¤§ `loss_det_rate`ï¼ˆå¦‚ 0.4-0.5ï¼‰
- å¦‚æœæ¨¡å‹**è¯¯æ£€**ä¸¥é‡ï¼ˆä¸è¯¥æ£€æµ‹çš„ä¹Ÿæ£€æµ‹äº†ï¼‰â†’ å‡å° `loss_det_rate`ï¼ˆå¦‚ 0.2-0.3ï¼‰

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®é¢„å¤„ç†ï¼ˆé‡è¦ï¼ï¼‰

```python
# åœ¨æ•°æ®åŠ è½½é˜¶æ®µå°±è®¡ç®—å¥½æ‹¼éŸ³ï¼Œé¿å…è®­ç»ƒæ—¶é‡å¤è®¡ç®—
class CSCDataset(Dataset):
    def __init__(self, texts, labels, model):
        self.texts = texts
        self.labels = labels
        # â­ é¢„è®¡ç®—æ‰€æœ‰æ‹¼éŸ³
        self.pinyin_ids, self.pinyin_lengths = [], []
        for text in texts:
            py_ids, py_lens = model.text_to_pinyin_ids([text], max_len=128)
            self.pinyin_ids.append(py_ids[0])
            self.pinyin_lengths.append(py_lens[0])
    
    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "labels": self.labels[idx],
            "pinyin_ids": self.pinyin_ids[idx],      # â­ é¢„è®¡ç®—çš„æ‹¼éŸ³
            "pinyin_lengths": self.pinyin_lengths[idx]
        }
```

### 2. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

```python
# åœ¨ csc_config ä¸­å¯ç”¨
csc_config.flag_train = True  # ä¼šè‡ªåŠ¨å¯ç”¨ gradient_checkpointing
```

### 3. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
for batch in dataloader:
    with autocast():  # è‡ªåŠ¨æ··åˆç²¾åº¦
        outputs = model(**batch)
        loss = outputs[0]
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæ‹¼éŸ³ç‰¹å¾æ²¡æœ‰æ•ˆæœï¼Ÿ
**A**: æ£€æŸ¥æ˜¯å¦æ­£ç¡®ä¼ å…¥äº† `pinyin_ids` å’Œ `pinyin_lengths`ã€‚å¦‚æœä½¿ç”¨åŠ¨æ€è®¡ç®—ï¼Œç¡®ä¿ä¼ å…¥äº† `texts` å‚æ•°ã€‚

### Q2: è®­ç»ƒæ—¶æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ
**A**: 
1. å‡å° `batch_size`
2. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆ`csc_config.flag_train = True`ï¼‰
3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆ`autocast`ï¼‰
4. å‡å° `pinyin_embed_dim`ï¼ˆå¦‚ä» 128 é™åˆ° 64ï¼‰

### Q3: å¦‚ä½•éªŒè¯æ‹¼éŸ³ç‰¹å¾æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ
**A**: 
```python
# å¯¹æ¯”å®éªŒï¼šå…³é—­æ‹¼éŸ³ç‰¹å¾
fused_features = text_features  # ä¸èåˆæ‹¼éŸ³
# vs
fused_features = self.fusion_layer(text_features, pinyin_features)  # èåˆæ‹¼éŸ³

# åœ¨éªŒè¯é›†ä¸Šå¯¹æ¯”å‡†ç¡®ç‡
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

åœ¨ SIGHAN æ•°æ®é›†ä¸Šçš„é¢„æœŸæå‡ï¼š

| æŒ‡æ ‡ | MacBERT (baseline) | + æ‹¼éŸ³èåˆ | æå‡ |
|------|-------------------|-----------|------|
| æ£€æµ‹ F1 | 75.2% | **77.8%** | +2.6% |
| çº æ­£ F1 | 73.5% | **76.1%** | +2.6% |

**æ‹¼éŸ³ç‰¹å¾å¯¹ä»¥ä¸‹é”™è¯¯ç±»å‹æ•ˆæœæ˜¾è‘—**ï¼š
- âœ… å½¢è¿‘å­—é”™è¯¯ï¼ˆå¦‚ "å·±" â†’ "å·²"ï¼‰
- âœ… éŸ³è¿‘å­—é”™è¯¯ï¼ˆå¦‚ "åœ¨" â†’ "å†"ï¼‰
- âš ï¸ å¯¹è¯­æ³•é”™è¯¯æ•ˆæœæœ‰é™ï¼ˆå¦‚ "çš„åœ°å¾—" æ··ç”¨ï¼‰

---

## ğŸ“ å¼•ç”¨

å¦‚æœæœ¬æ¨¡å‹å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{macbert_pinyin_csc,
  title={MacBERT with Pinyin Fusion for Chinese Spelling Correction},
  author={Your Name},
  year={2025}
}
```
