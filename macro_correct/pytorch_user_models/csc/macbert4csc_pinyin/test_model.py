# !/usr/bin/python
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯æ‹¼éŸ³èåˆæ¨¡å‹çš„ç»´åº¦å¯¹é½ã€Maskå¤„ç†ã€æ¢¯åº¦å›ä¼ 
"""

import torch
import torch.nn as nn
from graph import Macbert4CSCWithPinyin, PinyinEncoder, FusionLayer


class MockConfig:
    """æ¨¡æ‹Ÿé…ç½®å¯¹è±¡"""
    def __init__(self):
        # â­ ä½¿ç”¨å·²ä¸‹è½½çš„ MacBERT æ¨¡å‹ï¼ˆä»ç¼“å­˜åŠ è½½ï¼‰
        self.pretrained_model_name_or_path = "hfl/chinese-macbert-base"
        
        # â­ è®¾ç½®é•œåƒç«™ï¼ˆç¡®ä¿èƒ½ä»ç¼“å­˜åŠ è½½ï¼‰
        import os
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        
        self.flag_train = False
        self.loss_det_rate = 0.3


def test_pinyin_encoder():
    """æµ‹è¯•1ï¼šPinyinEncoder çš„ Mask å¤„ç†"""
    print("=" * 80)
    print("æµ‹è¯•1ï¼šPinyinEncoder çš„ Mask å¤„ç†")
    print("=" * 80)
    
    encoder = PinyinEncoder(pinyin_vocab_size=500, pinyin_embed_dim=128, hidden_size=768)
    
    # æ¨¡æ‹Ÿè¾“å…¥ï¼š[batch=2, seq_len=5, pinyin_len=8]
    batch_size, seq_len, pinyin_len = 2, 5, 8
    pinyin_ids = torch.randint(0, 50, (batch_size, seq_len, pinyin_len))
    pinyin_lengths = torch.tensor([
        [5, 6, 4, 0, 3],  # ç¬¬1ä¸ªæ ·æœ¬çš„5ä¸ªæ‹¼éŸ³çš„å®é™…é•¿åº¦
        [7, 5, 6, 4, 0]   # ç¬¬2ä¸ªæ ·æœ¬çš„5ä¸ªæ‹¼éŸ³çš„å®é™…é•¿åº¦
    ])
    
    # ä¸ä½¿ç”¨ Maskï¼ˆæ—§ç‰ˆæœ¬ï¼‰
    features_no_mask = encoder(pinyin_ids, pinyin_lengths=None)
    print(f"âœ… ä¸ä½¿ç”¨Maskçš„è¾“å‡ºå½¢çŠ¶: {features_no_mask.shape}")
    
    # ä½¿ç”¨ Maskï¼ˆæ–°ç‰ˆæœ¬ï¼‰
    features_with_mask = encoder(pinyin_ids, pinyin_lengths=pinyin_lengths)
    print(f"âœ… ä½¿ç”¨Maskçš„è¾“å‡ºå½¢çŠ¶: {features_with_mask.shape}")
    
    # éªŒè¯ç»´åº¦
    assert features_no_mask.shape == (batch_size, seq_len, 768), "ç»´åº¦ä¸åŒ¹é…ï¼"
    assert features_with_mask.shape == (batch_size, seq_len, 768), "ç»´åº¦ä¸åŒ¹é…ï¼"
    
    # éªŒè¯ä¸¤è€…æœ‰å·®å¼‚ï¼ˆè¯´æ˜Maskç”Ÿæ•ˆäº†ï¼‰
    diff = torch.abs(features_no_mask - features_with_mask).mean().item()
    print(f"âœ… ä¸¤ç§æ–¹å¼çš„ç‰¹å¾å·®å¼‚: {diff:.6f} (>0 è¯´æ˜Maskç”Ÿæ•ˆ)")
    assert diff > 0, "Maskæ²¡æœ‰ç”Ÿæ•ˆï¼"
    
    print("âœ… æµ‹è¯•1é€šè¿‡ï¼šPinyinEncoder çš„ Mask å¤„ç†æ­£ç¡®\n")


def test_fusion_layer():
    """æµ‹è¯•2ï¼šFusionLayer çš„èåˆé€»è¾‘å’Œæ¢¯åº¦å›ä¼ """
    print("=" * 80)
    print("æµ‹è¯•2ï¼šFusionLayer çš„èåˆé€»è¾‘å’Œæ¢¯åº¦å›ä¼ ")
    print("=" * 80)
    
    batch_size, seq_len, hidden_size = 2, 10, 768
    
    # æ¨¡æ‹Ÿè¾“å…¥
    text_features = torch.randn(batch_size, seq_len, hidden_size, requires_grad=True)
    pinyin_features = torch.randn(batch_size, seq_len, hidden_size, requires_grad=True)
    attention_mask = torch.ones(batch_size, seq_len)
    
    # æµ‹è¯•ä¸åŒèåˆæ–¹å¼
    for fusion_type in ["gate", "attention", "bilinear", "add"]:
        print(f"\n--- æµ‹è¯•èåˆæ–¹å¼: {fusion_type} ---")
        fusion_layer = FusionLayer(hidden_size=hidden_size, fusion_type=fusion_type)
        
        # å‰å‘ä¼ æ’­
        fused = fusion_layer(text_features, pinyin_features, attention_mask)
        print(f"âœ… èåˆåå½¢çŠ¶: {fused.shape}")
        assert fused.shape == (batch_size, seq_len, hidden_size), "ç»´åº¦ä¸åŒ¹é…ï¼"
        
        # åå‘ä¼ æ’­ï¼ˆéªŒè¯æ¢¯åº¦ï¼‰
        loss = fused.sum()
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
        assert text_features.grad is not None, "text_features æ¢¯åº¦ä¸ºç©ºï¼"
        assert pinyin_features.grad is not None, "pinyin_features æ¢¯åº¦ä¸ºç©ºï¼"
        print(f"âœ… text_features æ¢¯åº¦èŒƒæ•°: {text_features.grad.norm().item():.6f}")
        print(f"âœ… pinyin_features æ¢¯åº¦èŒƒæ•°: {pinyin_features.grad.norm().item():.6f}")
        
        # æ¸…ç©ºæ¢¯åº¦
        text_features.grad = None
        pinyin_features.grad = None
    
    print("\nâœ… æµ‹è¯•2é€šè¿‡ï¼šFusionLayer çš„èåˆé€»è¾‘å’Œæ¢¯åº¦å›ä¼ æ­£ç¡®\n")


def test_full_model():
    """æµ‹è¯•3ï¼šå®Œæ•´æ¨¡å‹çš„ç»´åº¦å¯¹é½"""
    print("=" * 80)
    print("æµ‹è¯•3ï¼šå®Œæ•´æ¨¡å‹çš„ç»´åº¦å¯¹é½")
    print("=" * 80)
    
    config = MockConfig()
    
    # â­ å¦‚æœæ²¡æœ‰é…ç½®æ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡æ­¤æµ‹è¯•
    if config.pretrained_model_name_or_path is None:
        print("âš ï¸ æœªé…ç½®é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡å®Œæ•´æ¨¡å‹æµ‹è¯•")
        print("ğŸ’¡ æç¤ºï¼šå¦‚æœéœ€è¦æµ‹è¯•å®Œæ•´æ¨¡å‹ï¼Œè¯·åœ¨ MockConfig ä¸­é…ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„")
        print("è·³è¿‡æµ‹è¯•3\n")
        return
    
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦çœŸå®çš„ MacBERT æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰ä¼šæŠ¥é”™
    try:
        model = Macbert4CSCWithPinyin(config, csc_config=config)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„")
        print("è·³è¿‡æµ‹è¯•3\n")
        return
    
    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size, seq_len = 2, 20
    input_ids = torch.randint(100, 5000, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)
    labels = torch.randint(100, 5000, (batch_size, seq_len))
    
    # æ–¹å¼1ï¼šé¢„è®¡ç®—æ‹¼éŸ³
    texts = ["æˆ‘çˆ±ä¸­å›½äººæ°‘", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"]
    pinyin_ids, pinyin_lengths = model.text_to_pinyin_ids(texts, seq_len)
    print(f"âœ… æ‹¼éŸ³IDå½¢çŠ¶: {pinyin_ids.shape}")
    print(f"âœ… æ‹¼éŸ³é•¿åº¦å½¢çŠ¶: {pinyin_lengths.shape}")
    
    # å‰å‘ä¼ æ’­
    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        pinyin_ids=pinyin_ids,
        pinyin_lengths=pinyin_lengths
    )
    
    loss, det_loss, cor_loss, det_probs, cor_probs, pred_ids = outputs
    print(f"âœ… æ€»æŸå¤±: {loss.item():.4f}")
    print(f"âœ… æ£€æµ‹æŸå¤±: {det_loss.item():.4f}")
    print(f"âœ… çº æ­£æŸå¤±: {cor_loss.item():.4f}")
    print(f"âœ… æ£€æµ‹æ¦‚ç‡å½¢çŠ¶: {det_probs.shape}")
    print(f"âœ… çº æ­£æ¦‚ç‡å½¢çŠ¶: {cor_probs.shape}")
    print(f"âœ… é¢„æµ‹IDå½¢çŠ¶: {pred_ids.shape}")
    
    # éªŒè¯ç»´åº¦
    assert det_probs.shape == (batch_size, seq_len), "æ£€æµ‹æ¦‚ç‡ç»´åº¦ä¸åŒ¹é…ï¼"
    assert cor_probs.shape[:-1] == (batch_size, seq_len), "çº æ­£æ¦‚ç‡ç»´åº¦ä¸åŒ¹é…ï¼"
    assert pred_ids.shape == (batch_size, seq_len), "é¢„æµ‹IDç»´åº¦ä¸åŒ¹é…ï¼"
    
    # æ–¹å¼2ï¼šåŠ¨æ€è®¡ç®—æ‹¼éŸ³
    outputs2 = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        texts=texts  # ç›´æ¥ä¼ å…¥æ–‡æœ¬
    )
    loss2 = outputs2[0]
    print(f"âœ… åŠ¨æ€è®¡ç®—æ‹¼éŸ³çš„æŸå¤±: {loss2.item():.4f}")
    
    print("\nâœ… æµ‹è¯•3é€šè¿‡ï¼šå®Œæ•´æ¨¡å‹çš„ç»´åº¦å¯¹é½æ­£ç¡®\n")


def test_gradient_flow():
    """æµ‹è¯•4ï¼šéªŒè¯æ¢¯åº¦èƒ½åŒæ—¶æ›´æ–° BERT å’Œ GRU"""
    print("=" * 80)
    print("æµ‹è¯•4ï¼šéªŒè¯æ¢¯åº¦èƒ½åŒæ—¶æ›´æ–° BERT å’Œ GRU")
    print("=" * 80)
    
    config = MockConfig()
    
    # â­ å¦‚æœæ²¡æœ‰é…ç½®æ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡æ­¤æµ‹è¯•
    if config.pretrained_model_name_or_path is None:
        print("âš ï¸ æœªé…ç½®é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡æ¢¯åº¦æµ‹è¯•")
        print("ğŸ’¡ æç¤ºï¼šå¦‚æœéœ€è¦æµ‹è¯•æ¢¯åº¦æµï¼Œè¯·åœ¨ MockConfig ä¸­é…ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„")
        print("è·³è¿‡æµ‹è¯•4\n")
        return
    
    try:
        model = Macbert4CSCWithPinyin(config, csc_config=config)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„")
        print("è·³è¿‡æµ‹è¯•4\n")
        return
    
    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size, seq_len = 2, 10
    input_ids = torch.randint(100, 5000, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)
    labels = torch.randint(100, 5000, (batch_size, seq_len))
    texts = ["æˆ‘çˆ±ä¸­å›½", "ä»Šå¤©å¤©æ°”"]
    
    # å‰å‘ä¼ æ’­
    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        texts=texts
    )
    loss = outputs[0]
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥ BERT çš„æ¢¯åº¦
    bert_has_grad = False
    for name, param in model.bert.named_parameters():
        if param.grad is not None and param.grad.abs().sum() > 0:
            bert_has_grad = True
            print(f"âœ… BERT å‚æ•° {name} æœ‰æ¢¯åº¦: {param.grad.norm().item():.6f}")
            break
    
    # æ£€æŸ¥ PinyinEncoder çš„æ¢¯åº¦
    pinyin_has_grad = False
    for name, param in model.pinyin_encoder.named_parameters():
        if param.grad is not None and param.grad.abs().sum() > 0:
            pinyin_has_grad = True
            print(f"âœ… PinyinEncoder å‚æ•° {name} æœ‰æ¢¯åº¦: {param.grad.norm().item():.6f}")
            break
    
    # æ£€æŸ¥ FusionLayer çš„æ¢¯åº¦
    fusion_has_grad = False
    for name, param in model.fusion_layer.named_parameters():
        if param.grad is not None and param.grad.abs().sum() > 0:
            fusion_has_grad = True
            print(f"âœ… FusionLayer å‚æ•° {name} æœ‰æ¢¯åº¦: {param.grad.norm().item():.6f}")
            break
    
    assert bert_has_grad, "BERT æ²¡æœ‰æ¢¯åº¦ï¼"
    assert pinyin_has_grad, "PinyinEncoder æ²¡æœ‰æ¢¯åº¦ï¼"
    assert fusion_has_grad, "FusionLayer æ²¡æœ‰æ¢¯åº¦ï¼"
    
    print("\nâœ… æµ‹è¯•4é€šè¿‡ï¼šæ¢¯åº¦èƒ½åŒæ—¶æ›´æ–° BERTã€GRU å’Œ FusionLayer\n")


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("å¼€å§‹æµ‹è¯•æ‹¼éŸ³èåˆæ¨¡å‹")
    print("=" * 80 + "\n")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_pinyin_encoder()
    test_fusion_layer()
    test_full_model()
    test_gradient_flow()
    
    print("=" * 80)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å®ç°æ­£ç¡®ï¼")
    print("=" * 80)
