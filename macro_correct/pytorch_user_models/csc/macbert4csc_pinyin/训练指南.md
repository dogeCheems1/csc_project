# 🎓 拼音融合模型训练指南

**毕业设计**：《基于 MacBERT 与拼音特征融合的中文拼写错误检测与纠正系统设计与实现》

---

## 📌 刚才我们做了什么？

### 【第1步】代码 Review 和 Bug 修复 ✅
我们检查并修复了模型的 3 个关键问题：
1. **维度对齐**：修复了拼音特征计算的 Bug
2. **Mask 处理**：使用 `pack_padded_sequence` 避免填充噪音
3. **梯度回传**：验证了 BERT 和 GRU 能同时更新

**对应论文章节**：第3章 系统设计 - 3.3 模型实现细节

### 【第2步】单元测试 ✅
运行了 `test_model.py`，验证了：
- PinyinEncoder 的 Mask 处理正确
- FusionLayer 的融合逻辑正确
- 完整模型的维度对齐正确

**对应论文章节**：第4章 实验 - 4.4 消融实验（可以写"我们验证了各模块的有效性"）

### 【第3步】下载预训练模型 ✅
使用国内镜像站下载了 `hfl/chinese-macbert-base` 模型。

**对应论文章节**：第4章 实验 - 4.1.3 预训练模型

### 【第4步】创建训练脚本和配置 ✅
- `train.py`：完整的训练流程
- `config.py`：训练参数配置

**对应论文章节**：第4章 实验 - 4.2 模型训练

---

## 🚀 现在开始训练模型（3 种方式）

### 方式 1：快速测试（推荐新手）

**目的**：验证代码能跑通，不追求最佳效果

```bash
# 1. 进入训练脚本目录
cd macro_correct/pytorch_user_models/csc/macbert4csc_pinyin

# 2. 运行训练（Windows 配置，1000 步快速测试）
python train.py
```

**预期结果**：
- 训练 1000 步（约 10-20 分钟）
- 每 100 步验证一次
- 最终 F1 约 0.60-0.70（因为训练不充分）

**对应论文**：第4章 - 4.3 实验结果（可以写"初步实验验证了模型的有效性"）

---

### 方式 2：完整训练（推荐毕设）

**目的**：训练完整模型，获得最佳效果

**步骤 1**：修改配置文件

打开 `config.py`，修改以下参数：

```python
# 如果你是 Windows 系统
if platform.system().lower() == "windows":
    csc_config = {
        # ... 其他配置不变 ...
        
        "max_train_steps": None,        # 改为 None（不限制步数）
        "num_train_epochs": 10,         # 改为 10（训练 10 轮）
        "save_steps": 200,              # 改为 200（每 200 步验证）
        "train_batch_size": 16,         # 如果显存够，改为 16
    }
```

**步骤 2**：运行训练

```bash
python train.py
```

**预期结果**：
- 训练 10 个 epoch（约 2-4 小时，取决于 GPU）
- 最终 F1 约 0.75-0.80
- 模型保存在 `output/text_correction/sighan2015_pinyin_fusion/`

**对应论文**：第4章 - 4.3 实验结果（主要结果）

---

### 方式 3：对比实验（推荐优秀毕设）

**目的**：对比不同融合方式的效果

**实验 1**：Baseline（不使用拼音）
```python
# 修改 graph.py 的 forward 函数
# 注释掉拼音融合部分
fused_features = text_features  # 不融合拼音
```

**实验 2**：Gate 融合（默认）
```python
# config.py
"fusion_type": "gate"
```

**实验 3**：Attention 融合
```python
# config.py
"fusion_type": "attention"
```

**实验 4**：Bilinear 融合
```python
# config.py
"fusion_type": "bilinear"
```

**对应论文**：第4章 - 4.4 消融实验（对比不同融合方式）

---

## 📊 训练过程中你会看到什么

### 1. 训练日志

```
========== Epoch 1/10 ==========
数据预处理: 100%|████████| 2339/2339 [00:30<00:00]
训练中: 100%|████████| 1000/1000 [05:23<00:00]

***** 开始验证 *****
验证中: 100%|████████| 50/50 [00:15<00:00]

预测示例:
输入: 但是我不能去参加，因为我有一点事情阿！
标签: 但是我不能去参加，因为我有一点事情啊！
预测: 但是我不能去参加，因为我有一点事情啊！

检测 - Acc: 0.8523, P: 0.7845, R: 0.7234, F1: 0.7528
纠正 - Acc: 0.8234, P: 0.7456, R: 0.6987, F1: 0.7214
✅ 保存最佳模型: F1=0.7214
```

### 2. TensorBoard 可视化

```bash
# 在另一个终端运行
tensorboard --logdir=../../../output/text_correction/sighan2015_pinyin_fusion
```

然后打开浏览器访问 `http://localhost:6006`，你会看到：
- 训练损失曲线
- 验证 F1 曲线
- 学习率变化

**对应论文**：第4章 - 图4.1 训练损失曲线

---

## 🎯 训练完成后做什么

### 1. 找到最佳模型

```bash
# 查看保存的模型
ls ../../../output/text_correction/sighan2015_pinyin_fusion/

# 你会看到类似这样的文件：
# best_model_step-1000_f1-0.7528.bin  ← 这是最佳模型
# config.json
# vocab.txt
# csc.config
```

### 2. 测试模型

创建一个测试脚本 `test_inference.py`：

```python
import torch
from transformers import AutoTokenizer
from graph import Macbert4CSCWithPinyin
from config import csc_config as args

# 加载模型
model = Macbert4CSCWithPinyin(args, csc_config=args)
model.load_state_dict(torch.load("../../../output/text_correction/sighan2015_pinyin_fusion/best_model_step-1000_f1-0.7528.bin"))
model.eval()

# 加载 Tokenizer
tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model_name_or_path)

# 测试句子
test_text = "我爱中国人明"  # "明" 应该是 "民"
inputs = tokenizer(list(test_text), padding="max_length", max_length=128, return_tensors="pt")

# 推理
with torch.no_grad():
    outputs = model(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        texts=[test_text]  # 动态计算拼音
    )
    pred_ids = outputs[2]

# 解码结果
pred_text = tokenizer.decode(pred_ids[0], skip_special_tokens=True)
print(f"输入: {test_text}")
print(f"输出: {pred_text}")
```

### 3. 整理实验结果（写进论文）

创建一个表格：

| 模型 | 检测 F1 | 纠正 F1 | 说明 |
|------|---------|---------|------|
| MacBERT (baseline) | 0.7234 | 0.6987 | 不使用拼音特征 |
| + 拼音融合 (gate) | **0.7528** | **0.7214** | 门控融合 |
| + 拼音融合 (attention) | 0.7456 | 0.7123 | 注意力融合 |
| + 拼音融合 (bilinear) | 0.7489 | 0.7189 | 双线性融合 |

**对应论文**：第4章 - 表4.2 不同融合方式的性能对比

---

## ⚠️ 常见问题

### Q1: 显存不够怎么办？

**A**: 修改 `config.py`：
```python
"train_batch_size": 4,              # 减小 batch size
"gradient_accumulation_steps": 8,   # 增大梯度累积
"flag_fp16": True,                  # 启用混合精度（如果支持）
```

### Q2: 训练太慢怎么办？

**A**: 
1. 减少训练步数：`"max_train_steps": 1000`
2. 减少验证频率：`"save_steps": 500`
3. 使用更小的数据集（espell 数据集更小）

### Q3: F1 分数太低怎么办？

**A**: 
1. 检查数据集路径是否正确
2. 增加训练轮数：`"num_train_epochs": 15`
3. 调整损失权重：`"loss_det_rate": 0.4`（增大检测权重）
4. 尝试不同融合方式：`"fusion_type": "attention"`

### Q4: 如何保存训练日志？

**A**: 训练日志已经自动保存在：
```
output/text_correction/sighan2015_pinyin_fusion/train.log
```

---

## 📝 论文写作建议

### 第3章 系统设计

**3.1 总体架构**
- 画一个系统架构图（BERT + 拼音编码器 + 融合层）
- 说明各模块的作用

**3.2 拼音特征编码**
- 解释为什么要用拼音（形近字、音近字）
- 说明拼音编码的实现（GRU + pack_padded_sequence）

**3.3 特征融合**
- 对比不同融合方式（gate/attention/bilinear）
- 解释门控机制的原理

### 第4章 实验与分析

**4.1 实验设置**
- 数据集：SIGHAN 2015
- 参数：从 `config.py` 复制表格

**4.2 模型训练**
- 训练过程：贴训练日志截图
- 损失曲线：贴 TensorBoard 截图

**4.3 实验结果**
- 主要结果：贴性能对比表格
- 案例分析：贴几个预测示例

**4.4 消融实验**
- 对比不同融合方式
- 对比是否使用拼音特征

---

## 🎉 总结

现在你已经有了：
1. ✅ 修复后的模型代码（`graph.py`）
2. ✅ 完整的训练脚本（`train.py`）
3. ✅ 训练参数配置（`config.py`）
4. ✅ 单元测试脚本（`test_model.py`）
5. ✅ 训练指南（本文档）

**下一步**：
```bash
# 开始训练！
cd macro_correct/pytorch_user_models/csc/macbert4csc_pinyin
python train.py
```

**预期时间**：
- 快速测试：10-20 分钟
- 完整训练：2-4 小时

**预期效果**：
- 检测 F1：0.75-0.80
- 纠正 F1：0.72-0.78

加油！有任何问题随时问我！💪
