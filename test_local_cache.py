#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯èƒ½å¦ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹
"""

import os

# â­ æ¸…é™¤ä»£ç†è®¾ç½®
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)

# è®¾ç½®ç¦»çº¿æ¨¡å¼
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'

print("=" * 80)
print("æµ‹è¯•ï¼šä»æœ¬åœ°ç¼“å­˜åŠ è½½ MacBERT æ¨¡å‹")
print("=" * 80)

try:
    from transformers import AutoTokenizer, BertForMaskedLM
    
    print("\n1ï¸âƒ£ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        "hfl/chinese-macbert-base",
        local_files_only=True  # åªä½¿ç”¨æœ¬åœ°ç¼“å­˜
    )
    print("   âœ… Tokenizer åŠ è½½æˆåŠŸ")
    print(f"   è¯è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    print("\n2ï¸âƒ£ åŠ è½½æ¨¡å‹...")
    model = BertForMaskedLM.from_pretrained(
        "hfl/chinese-macbert-base",
        local_files_only=True  # åªä½¿ç”¨æœ¬åœ°ç¼“å­˜
    )
    print("   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"   éšè—å±‚ç»´åº¦: {model.config.hidden_size}")
    
    print("\n3ï¸âƒ£ æµ‹è¯•æ¨ç†...")
    test_text = "æˆ‘çˆ±ä¸­å›½"
    inputs = tokenizer(test_text, return_tensors="pt")
    outputs = model(**inputs)
    print(f"   âœ… æ¨ç†æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æµ‹è¯•é€šè¿‡ï¼å¯ä»¥ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹")
    print("=" * 80)
    print("\nç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒè„šæœ¬äº†ï¼š")
    print("python macro_correct/pytorch_user_models/csc/macbert4csc_pinyin/run_ablation_study.py")
    
except Exception as e:
    print("\n" + "=" * 80)
    print("âŒ æµ‹è¯•å¤±è´¥")
    print("=" * 80)
    print(f"é”™è¯¯ä¿¡æ¯: {e}")
    print("\nå¯èƒ½çš„åŸå› ï¼š")
    print("1. æœ¬åœ°ç¼“å­˜ä¸­æ²¡æœ‰æ¨¡å‹ï¼ˆéœ€è¦å…ˆä¸‹è½½ï¼‰")
    print("2. ä»£ç†è®¾ç½®ä»ç„¶æœ‰é—®é¢˜")
    print("\nè§£å†³æ–¹æ¡ˆï¼š")
    print("1. è¿è¡Œä¸‹è½½è„šæœ¬ï¼špython download_macbert.py")
    print("2. æˆ–è€…æ‰‹åŠ¨å…³é—­ç³»ç»Ÿä»£ç†åå†è¯•")
